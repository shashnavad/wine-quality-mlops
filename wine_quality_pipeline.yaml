# PIPELINE DEFINITION
# Name: wine-quality-pipeline
# Description: End-to-end ML pipeline for wine quality prediction with model selection
# Inputs:
#    data_path: str [Default: 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv']
#    lgbm_learning_rate: float [Default: 0.1]
#    lgbm_max_depth: int [Default: -1.0]
#    lgbm_n_estimators: int [Default: 100.0]
#    rf_max_depth: int
#    rf_min_samples_split: int [Default: 2.0]
#    rf_n_estimators: int [Default: 100.0]
#    service_name: str [Default: 'wine-quality-predictor']
#    use_lightgbm: bool [Default: True]
#    use_random_forest: bool [Default: True]
#    use_xgboost: bool [Default: True]
#    xgb_learning_rate: float [Default: 0.1]
#    xgb_max_depth: int [Default: 6.0]
#    xgb_n_estimators: int [Default: 100.0]
components:
  comp-condition-1:
    dag:
      tasks:
        deploy-model:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-deploy-model
          dependentTasks:
          - preprocess
          - select-best-model
          inputs:
            artifacts:
              metrics:
                taskOutputArtifact:
                  outputArtifactKey: best_metrics
                  producerTask: select-best-model
              model:
                taskOutputArtifact:
                  outputArtifactKey: best_model
                  producerTask: select-best-model
              scaler:
                taskOutputArtifact:
                  outputArtifactKey: scaler
                  producerTask: preprocess
            parameters:
              service_name:
                componentInputParameter: pipelinechannel--service_name
          taskInfo:
            name: deploy-model
        preprocess:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-preprocess
          inputs:
            parameters:
              data_path:
                componentInputParameter: pipelinechannel--data_path
          taskInfo:
            name: preprocess
        select-best-model:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-select-best-model
          dependentTasks:
          - train
          - train-2
          - train-3
          inputs:
            artifacts:
              metrics1:
                taskOutputArtifact:
                  outputArtifactKey: metrics
                  producerTask: train
              metrics2:
                taskOutputArtifact:
                  outputArtifactKey: metrics
                  producerTask: train-2
              metrics3:
                taskOutputArtifact:
                  outputArtifactKey: metrics
                  producerTask: train-3
              model1:
                taskOutputArtifact:
                  outputArtifactKey: model
                  producerTask: train
              model2:
                taskOutputArtifact:
                  outputArtifactKey: model
                  producerTask: train-2
              model3:
                taskOutputArtifact:
                  outputArtifactKey: model
                  producerTask: train-3
          taskInfo:
            name: select-best-model
        train:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-train
          dependentTasks:
          - preprocess
          inputs:
            artifacts:
              features:
                taskOutputArtifact:
                  outputArtifactKey: features
                  producerTask: preprocess
              labels:
                taskOutputArtifact:
                  outputArtifactKey: labels
                  producerTask: preprocess
              scaler:
                taskOutputArtifact:
                  outputArtifactKey: scaler
                  producerTask: preprocess
            parameters:
              hyperparameters:
                runtimeValue:
                  constant:
                    max_depth: '{{$.inputs.parameters[''pipelinechannel--rf_max_depth'']}}'
                    min_samples_split: '{{$.inputs.parameters[''pipelinechannel--rf_min_samples_split'']}}'
                    n_estimators: '{{$.inputs.parameters[''pipelinechannel--rf_n_estimators'']}}'
                    random_state: 42.0
              model_type:
                runtimeValue:
                  constant: RandomForest
              pipelinechannel--rf_max_depth:
                componentInputParameter: pipelinechannel--rf_max_depth
              pipelinechannel--rf_min_samples_split:
                componentInputParameter: pipelinechannel--rf_min_samples_split
              pipelinechannel--rf_n_estimators:
                componentInputParameter: pipelinechannel--rf_n_estimators
          taskInfo:
            name: train
        train-2:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-train-2
          dependentTasks:
          - preprocess
          inputs:
            artifacts:
              features:
                taskOutputArtifact:
                  outputArtifactKey: features
                  producerTask: preprocess
              labels:
                taskOutputArtifact:
                  outputArtifactKey: labels
                  producerTask: preprocess
              scaler:
                taskOutputArtifact:
                  outputArtifactKey: scaler
                  producerTask: preprocess
            parameters:
              hyperparameters:
                runtimeValue:
                  constant:
                    learning_rate: '{{$.inputs.parameters[''pipelinechannel--xgb_learning_rate'']}}'
                    max_depth: '{{$.inputs.parameters[''pipelinechannel--xgb_max_depth'']}}'
                    n_estimators: '{{$.inputs.parameters[''pipelinechannel--xgb_n_estimators'']}}'
                    random_state: 42.0
              model_type:
                runtimeValue:
                  constant: XGBoost
              pipelinechannel--xgb_learning_rate:
                componentInputParameter: pipelinechannel--xgb_learning_rate
              pipelinechannel--xgb_max_depth:
                componentInputParameter: pipelinechannel--xgb_max_depth
              pipelinechannel--xgb_n_estimators:
                componentInputParameter: pipelinechannel--xgb_n_estimators
          taskInfo:
            name: train-2
        train-3:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-train-3
          dependentTasks:
          - preprocess
          inputs:
            artifacts:
              features:
                taskOutputArtifact:
                  outputArtifactKey: features
                  producerTask: preprocess
              labels:
                taskOutputArtifact:
                  outputArtifactKey: labels
                  producerTask: preprocess
              scaler:
                taskOutputArtifact:
                  outputArtifactKey: scaler
                  producerTask: preprocess
            parameters:
              hyperparameters:
                runtimeValue:
                  constant:
                    learning_rate: '{{$.inputs.parameters[''pipelinechannel--lgbm_learning_rate'']}}'
                    max_depth: '{{$.inputs.parameters[''pipelinechannel--lgbm_max_depth'']}}'
                    n_estimators: '{{$.inputs.parameters[''pipelinechannel--lgbm_n_estimators'']}}'
                    random_state: 42.0
              model_type:
                runtimeValue:
                  constant: LightGBM
              pipelinechannel--lgbm_learning_rate:
                componentInputParameter: pipelinechannel--lgbm_learning_rate
              pipelinechannel--lgbm_max_depth:
                componentInputParameter: pipelinechannel--lgbm_max_depth
              pipelinechannel--lgbm_n_estimators:
                componentInputParameter: pipelinechannel--lgbm_n_estimators
          taskInfo:
            name: train-3
    inputDefinitions:
      parameters:
        pipelinechannel--data_path:
          parameterType: STRING
        pipelinechannel--lgbm_learning_rate:
          parameterType: NUMBER_DOUBLE
        pipelinechannel--lgbm_max_depth:
          parameterType: NUMBER_INTEGER
        pipelinechannel--lgbm_n_estimators:
          parameterType: NUMBER_INTEGER
        pipelinechannel--rf_max_depth:
          parameterType: NUMBER_INTEGER
        pipelinechannel--rf_min_samples_split:
          parameterType: NUMBER_INTEGER
        pipelinechannel--rf_n_estimators:
          parameterType: NUMBER_INTEGER
        pipelinechannel--service_name:
          parameterType: STRING
        pipelinechannel--validate-data-validation_success:
          parameterType: BOOLEAN
        pipelinechannel--xgb_learning_rate:
          parameterType: NUMBER_DOUBLE
        pipelinechannel--xgb_max_depth:
          parameterType: NUMBER_INTEGER
        pipelinechannel--xgb_n_estimators:
          parameterType: NUMBER_INTEGER
  comp-deploy-model:
    executorLabel: exec-deploy-model
    inputDefinitions:
      artifacts:
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        scaler:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        namespace:
          defaultValue: kubeflow
          isOptional: true
          parameterType: STRING
        service_name:
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-preprocess:
    executorLabel: exec-preprocess
    inputDefinitions:
      parameters:
        data_path:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        features:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        labels:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        scaler:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-select-best-model:
    executorLabel: exec-select-best-model
    inputDefinitions:
      artifacts:
        metrics1:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
          isOptional: true
        metrics2:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
          isOptional: true
        metrics3:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
          isOptional: true
        model1:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
          isOptional: true
        model2:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
          isOptional: true
        model3:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
          isOptional: true
    outputDefinitions:
      artifacts:
        best_metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        best_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-train:
    executorLabel: exec-train
    inputDefinitions:
      artifacts:
        features:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        labels:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        scaler:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        hyperparameters:
          parameterType: STRUCT
        model_type:
          defaultValue: RandomForest
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      artifacts:
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-train-2:
    executorLabel: exec-train-2
    inputDefinitions:
      artifacts:
        features:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        labels:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        scaler:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        hyperparameters:
          parameterType: STRUCT
        model_type:
          defaultValue: RandomForest
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      artifacts:
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-train-3:
    executorLabel: exec-train-3
    inputDefinitions:
      artifacts:
        features:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        labels:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        scaler:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        hyperparameters:
          parameterType: STRUCT
        model_type:
          defaultValue: RandomForest
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      artifacts:
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-validate-data:
    executorLabel: exec-validate-data
    inputDefinitions:
      parameters:
        data_path:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
      parameters:
        validation_success:
          parameterType: BOOLEAN
deploymentSpec:
  executors:
    exec-deploy-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - deploy_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'kserve==0.10.1'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef deploy_model(\n    model: Input[Model],\n    metrics: Input[Metrics],\n\
          \    scaler: Input[Model],\n    service_name: str,\n    namespace: str =\
          \ \"kubeflow\"\n) -> str:\n    import os\n    import json\n    import pickle\n\
          \    import tempfile\n    from kubernetes import client\n    from kubernetes\
          \ import config\n    from kserve import KServeClient\n    from kserve import\
          \ constants\n    from kserve import V1beta1InferenceService\n    from kserve\
          \ import V1beta1InferenceServiceSpec\n    from kserve import V1beta1PredictorSpec\n\
          \    from kserve import V1beta1SKLearnSpec\n\n    # Create a temporary directory\
          \ to prepare the model\n    model_dir = tempfile.mkdtemp()\n    os.makedirs(os.path.join(model_dir,\
          \ \"model\"), exist_ok=True)\n\n    # Load and save the model and scaler\n\
          \    with open(model.path, 'rb') as f:\n        model_obj = pickle.load(f)\n\
          \n    with open(scaler.path, 'rb') as f:\n        scaler_obj = pickle.load(f)\n\
          \n    # Save model and scaler to the temporary directory\n    with open(os.path.join(model_dir,\
          \ \"model\", \"model.pkl\"), 'wb') as f:\n        pickle.dump(model_obj,\
          \ f)\n\n    with open(os.path.join(model_dir, \"model\", \"scaler.pkl\"\
          ), 'wb') as f:\n        pickle.dump(scaler_obj, f)\n\n    # Load metrics\
          \ to include in model metadata\n    with open(metrics.path, 'r') as f:\n\
          \        metrics_data = json.load(f)\n\n    # Create a metadata file\n \
          \   metadata = {\n        \"name\": service_name,\n        \"version\":\
          \ \"v1\",\n        \"metrics\": metrics_data\n    }\n\n    with open(os.path.join(model_dir,\
          \ \"model\", \"metadata.json\"), 'w') as f:\n        json.dump(metadata,\
          \ f)\n\n    # Create a simple inference script\n    inference_script = \"\
          \"\"\nimport os\nimport pickle\nimport json\nimport numpy as np\n\nclass\
          \ WineQualityModel(object):\n    def __init__(self):\n        self.model\
          \ = None\n        self.scaler = None\n        self.ready = False\n\n   \
          \ def load(self):\n        model_dir = os.path.join(os.getcwd(), \"model\"\
          )\n        with open(os.path.join(model_dir, \"model.pkl\"), \"rb\") as\
          \ f:\n            self.model = pickle.load(f)\n        with open(os.path.join(model_dir,\
          \ \"scaler.pkl\"), \"rb\") as f:\n            self.scaler = pickle.load(f)\n\
          \        self.ready = True\n\n    def predict(self, X, feature_names=None):\n\
          \        if not self.ready:\n            self.load()\n        X_scaled =\
          \ self.scaler.transform(X)\n        predictions = self.model.predict(X_scaled)\n\
          \        return predictions.tolist()\n\"\"\"\n\n    with open(os.path.join(model_dir,\
          \ \"model\", \"WineQualityModel.py\"), 'w') as f:\n        f.write(inference_script)\n\
          \n    # Create a simple requirements.txt\n    with open(os.path.join(model_dir,\
          \ \"model\", \"requirements.txt\"), 'w') as f:\n        f.write(\"scikit-learn==1.0.2\\\
          nnumpy==1.22.3\\n\")\n\n    # Upload the model to a storage location (MinIO,\
          \ S3, etc.)\n    # Assuming you have a PVC for model storage\n    model_uri\
          \ = f\"pvc://{service_name}-models\"\n\n    # In a real implementation,\
          \ you would upload the model to your storage\n    # For now, printing the\
          \ path and assuming it's accessible\n    print(f\"Model prepared at: {model_dir}\"\
          )\n    print(f\"Model would be deployed from: {model_uri}\")\n\n    try:\n\
          \        # Initialize KServe client\n        config.load_incluster_config()\n\
          \        kserve_client = KServeClient()\n\n        # Define the inference\
          \ service\n        isvc = V1beta1InferenceService(\n            api_version=constants.KSERVE_V1BETA1,\n\
          \            kind=constants.KSERVE_KIND,\n            metadata=client.V1ObjectMeta(\n\
          \                name=service_name,\n                namespace=namespace,\n\
          \                annotations={\"sidecar.istio.io/inject\": \"false\"}\n\
          \            ),\n            spec=V1beta1InferenceServiceSpec(\n       \
          \         predictor=V1beta1PredictorSpec(\n                    sklearn=V1beta1SKLearnSpec(\n\
          \                        storage_uri=model_uri,\n                      \
          \  resources=client.V1ResourceRequirements(\n                          \
          \  requests={\"cpu\": \"100m\", \"memory\": \"1Gi\"},\n                \
          \            limits={\"cpu\": \"1\", \"memory\": \"2Gi\"}\n            \
          \            )\n                    )\n                )\n            )\n\
          \        )\n\n        # Create the inference service\n        kserve_client.create(isvc)\n\
          \        print(f\"Inference service '{service_name}' created in namespace\
          \ '{namespace}'\")\n\n        # Get the URL of the deployed service\n  \
          \      service_url = f\"http://{service_name}.{namespace}.svc.cluster.local/v1/models/{service_name}:predict\"\
          \n        return service_url\n\n    except Exception as e:\n        print(f\"\
          Error deploying model: {str(e)}\")\n        return f\"Deployment failed:\
          \ {str(e)}\"\n\n"
        image: pes1ug19cs601/wine-quality-mlops:latest
    exec-preprocess:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - preprocess
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef preprocess(\n    data_path: str,\n    features: Output[Dataset],\n\
          \    labels: Output[Dataset],\n    scaler: Output[Model]\n):\n    \"\"\"\
          Preprocess the wine quality data.\"\"\"\n    # Load data\n    df = pd.read_csv(data_path,\
          \ sep=\";\")\n\n    # Split features and labels\n    X = df.drop('quality',\
          \ axis=1)\n    y = df['quality']\n\n    # Scale features\n    scaler_obj\
          \ = StandardScaler()\n    X_scaled = scaler_obj.fit_transform(X)\n\n   \
          \ # Save processed data\n    np.save(features.path, X_scaled)\n    np.save(labels.path,\
          \ y.values)\n    joblib.dump(scaler_obj, scaler.path)\n\n    print(f\"Preprocessed\
          \ features saved to {features.path}\")\n    print(f\"Preprocessed labels\
          \ saved to {labels.path}\")\n    print(f\"Scaler saved to {scaler.path}\"\
          )\n\n"
        image: pes1ug19cs601/wine-quality-mlops:latest
    exec-select-best-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - select_best_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef select_best_model(\n    best_model: Output[Model],  \n    best_metrics:\
          \ Output[Metrics],  \n    model1: Input[Model] = None,  \n    model2: Input[Model]\
          \ = None, \n    model3: Input[Model] = None,  \n    metrics1: Input[Metrics]\
          \ = None,  \n    metrics2: Input[Metrics] = None,  \n    metrics3: Input[Metrics]\
          \ = None  \n):\n    \"\"\"Select the best model based on test R\xB2 score.\"\
          \"\"\n    import json\n    import shutil\n    import copy\n\n    best_score\
          \ = -float('inf')\n    best_model_idx = -1\n    all_metrics_summary = []\n\
          \n    # Create lists of non-None inputs\n    models = [m for m in [model1,\
          \ model2, model3] if m is not None]\n    metrics_files = [m for m in [metrics1,\
          \ metrics2, metrics3] if m is not None]\n\n    # Load all metrics and find\
          \ the best model\n    for i, metric_file in enumerate(metrics_files):\n\
          \        with open(metric_file.path, 'r') as f:\n            metric_data\
          \ = json.load(f)\n\n        # Create a simplified summary to avoid circular\
          \ references\n        metric_summary = {\n            \"model_type\": metric_data.get(\"\
          model_type\", \"Unknown\"),\n            \"test_r2\": float(metric_data.get(\"\
          test_r2\", -1.0)),\n            \"train_r2\": float(metric_data.get(\"train_r2\"\
          , -1.0))\n        }\n\n        all_metrics_summary.append(metric_summary)\n\
          \        test_r2 = metric_summary[\"test_r2\"]\n\n        if test_r2 > best_score:\n\
          \            best_score = test_r2\n            best_model_idx = i\n\n  \
          \  # Copy the best model and its metrics\n    if best_model_idx >= 0:\n\
          \        shutil.copy(models[best_model_idx].path, best_model.path)\n\n \
          \       # Create a new dictionary for best metrics to avoid reference issues\n\
          \        best_metric_data = copy.deepcopy(all_metrics_summary[best_model_idx])\n\
          \        best_metric_data[\"model_comparison\"] = [\n            {\"model\"\
          : m[\"model_type\"], \"test_r2\": m[\"test_r2\"]}\n            for m in\
          \ all_metrics_summary\n        ]\n\n        with open(best_metrics.path,\
          \ 'w') as f:\n            json.dump(best_metric_data, f, indent=2)\n\n \
          \       print(f\"Selected best model: {best_metric_data['model_type']}\"\
          )\n        print(f\"Best test R\xB2: {best_score:.4f}\")\n    else:\n  \
          \      raise ValueError(\"No valid models found\")\n\n"
        image: pes1ug19cs601/wine-quality-mlops:latest
    exec-train:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'xgboost' 'lightgbm'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train(\n    features: Input[Dataset],\n    labels: Input[Dataset],\n\
          \    hyperparameters: dict,\n    model: Output[Model],\n    metrics: Output[Metrics],\n\
          \    scaler: Input[Model],\n    model_type: str = \"RandomForest\"  # Default\
          \ argument moved to the end\n):\n    \"\"\"Train a model with support for\
          \ multiple algorithms.\"\"\"\n    import os\n    import joblib\n    import\
          \ json\n    import numpy as np\n    from sklearn.model_selection import\
          \ train_test_split\n    from sklearn.ensemble import RandomForestRegressor\n\
          \    import xgboost as xgb\n    import lightgbm as lgb\n    import mlflow\n\
          \    import time\n    from prometheus_client import start_http_server, Gauge\n\
          \n    # Load data\n    X = np.load(features.path)\n    y = np.load(labels.path)\n\
          \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(X,\
          \ y, test_size=0.2, random_state=42)\n\n    # Initialize model based on\
          \ model_type\n    print(f\"Training {model_type} model with hyperparameters:\
          \ {hyperparameters}\")\n    if model_type == \"RandomForest\":\n       \
          \ model_obj = RandomForestRegressor(**hyperparameters)\n    elif model_type\
          \ == \"XGBoost\":\n        model_obj = xgb.XGBRegressor(**hyperparameters)\n\
          \    elif model_type == \"LightGBM\":\n        model_obj = lgb.LGBMRegressor(**hyperparameters)\n\
          \    else:\n        raise ValueError(f\"Unsupported model type: {model_type}\"\
          )\n\n    start_time = time.time()\n    # Train model\n    model_obj.fit(X_train,\
          \ y_train)\n\n    # Evaluate model\n    train_score = model_obj.score(X_train,\
          \ y_train)\n    test_score = model_obj.score(X_test, y_test)\n\n    # Save\
          \ metrics\n    metrics_dict = {\n        'model_type': model_type,\n   \
          \     'train_r2': float(train_score),\n        'test_r2': float(test_score)\n\
          \    }\n\n    with open(metrics.path, 'w') as f:\n        json.dump(metrics_dict,\
          \ f, indent=2)\n\n    # Save model\n    joblib.dump(model_obj, model.path)\n\
          \n    # Only log to MLflow if not in testing mode and if MLflow server is\
          \ available\n    if not os.environ.get(\"TESTING\", \"False\").lower() ==\
          \ \"true\":\n        try:\n            # Set a timeout for MLflow connection\
          \ attempts\n            import socket\n            socket.setdefaulttimeout(5)\
          \  # 5 second timeout\n\n            # Try to connect to MLflow server\n\
          \            mlflow.set_tracking_uri('http://mlflow-service.mlops.svc.cluster.local:5000')\n\
          \            with mlflow.start_run():\n                mlflow.log_param(\"\
          model_type\", model_type)\n                mlflow.log_params(hyperparameters)\n\
          \                mlflow.log_metric(\"train_r2\", train_score)\n        \
          \        mlflow.log_metric(\"test_r2\", test_score)\n                mlflow.sklearn.log_model(model_obj,\
          \ \"model\")\n                mlflow.log_artifact(scaler.path, \"preprocessor\"\
          )\n                print(\"Successfully logged metrics to MLflow\")\n  \
          \      except Exception as e:\n            print(f\"MLflow logging failed\
          \ (this is expected in local environments): {e}\")\n            print(\"\
          Continuing without MLflow logging\")\n    else:\n        print(\"Testing\
          \ mode active, skipping MLflow logging\")\n\n    print(f\"Model saved to\
          \ {model.path}\")\n    print(f\"Metrics saved to {metrics.path}\")\n   \
          \ print(f\"Training metrics: {metrics_dict}\")\n    # Add Prometheus metrics\
          \ server (after model evaluation)\n    prom_port = int(os.getenv(\"PROMETHEUS_PORT\"\
          , 8000))\n    start_http_server(prom_port)\n\n    # Create Prometheus gauges\n\
          \    train_r2_gauge = Gauge('train_r2_score', 'Training R\xB2 score', ['model_type'])\n\
          \    test_r2_gauge = Gauge('test_r2_score', 'Test R\xB2 score', ['model_type'])\n\
          \    training_time_gauge = Gauge('training_time_seconds', 'Training duration')\n\
          \n    # Set metric values\n    train_r2_gauge.labels(model_type=model_type).set(train_score)\n\
          \    test_r2_gauge.labels(model_type=model_type).set(test_score)\n    training_time_gauge.set(time.time()\
          \ - start_time)\n\n"
        env:
        - name: PROMETHEUS_PORT
          value: '8000'
        image: pes1ug19cs601/wine-quality-mlops:latest
    exec-train-2:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'xgboost' 'lightgbm'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train(\n    features: Input[Dataset],\n    labels: Input[Dataset],\n\
          \    hyperparameters: dict,\n    model: Output[Model],\n    metrics: Output[Metrics],\n\
          \    scaler: Input[Model],\n    model_type: str = \"RandomForest\"  # Default\
          \ argument moved to the end\n):\n    \"\"\"Train a model with support for\
          \ multiple algorithms.\"\"\"\n    import os\n    import joblib\n    import\
          \ json\n    import numpy as np\n    from sklearn.model_selection import\
          \ train_test_split\n    from sklearn.ensemble import RandomForestRegressor\n\
          \    import xgboost as xgb\n    import lightgbm as lgb\n    import mlflow\n\
          \    import time\n    from prometheus_client import start_http_server, Gauge\n\
          \n    # Load data\n    X = np.load(features.path)\n    y = np.load(labels.path)\n\
          \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(X,\
          \ y, test_size=0.2, random_state=42)\n\n    # Initialize model based on\
          \ model_type\n    print(f\"Training {model_type} model with hyperparameters:\
          \ {hyperparameters}\")\n    if model_type == \"RandomForest\":\n       \
          \ model_obj = RandomForestRegressor(**hyperparameters)\n    elif model_type\
          \ == \"XGBoost\":\n        model_obj = xgb.XGBRegressor(**hyperparameters)\n\
          \    elif model_type == \"LightGBM\":\n        model_obj = lgb.LGBMRegressor(**hyperparameters)\n\
          \    else:\n        raise ValueError(f\"Unsupported model type: {model_type}\"\
          )\n\n    start_time = time.time()\n    # Train model\n    model_obj.fit(X_train,\
          \ y_train)\n\n    # Evaluate model\n    train_score = model_obj.score(X_train,\
          \ y_train)\n    test_score = model_obj.score(X_test, y_test)\n\n    # Save\
          \ metrics\n    metrics_dict = {\n        'model_type': model_type,\n   \
          \     'train_r2': float(train_score),\n        'test_r2': float(test_score)\n\
          \    }\n\n    with open(metrics.path, 'w') as f:\n        json.dump(metrics_dict,\
          \ f, indent=2)\n\n    # Save model\n    joblib.dump(model_obj, model.path)\n\
          \n    # Only log to MLflow if not in testing mode and if MLflow server is\
          \ available\n    if not os.environ.get(\"TESTING\", \"False\").lower() ==\
          \ \"true\":\n        try:\n            # Set a timeout for MLflow connection\
          \ attempts\n            import socket\n            socket.setdefaulttimeout(5)\
          \  # 5 second timeout\n\n            # Try to connect to MLflow server\n\
          \            mlflow.set_tracking_uri('http://mlflow-service.mlops.svc.cluster.local:5000')\n\
          \            with mlflow.start_run():\n                mlflow.log_param(\"\
          model_type\", model_type)\n                mlflow.log_params(hyperparameters)\n\
          \                mlflow.log_metric(\"train_r2\", train_score)\n        \
          \        mlflow.log_metric(\"test_r2\", test_score)\n                mlflow.sklearn.log_model(model_obj,\
          \ \"model\")\n                mlflow.log_artifact(scaler.path, \"preprocessor\"\
          )\n                print(\"Successfully logged metrics to MLflow\")\n  \
          \      except Exception as e:\n            print(f\"MLflow logging failed\
          \ (this is expected in local environments): {e}\")\n            print(\"\
          Continuing without MLflow logging\")\n    else:\n        print(\"Testing\
          \ mode active, skipping MLflow logging\")\n\n    print(f\"Model saved to\
          \ {model.path}\")\n    print(f\"Metrics saved to {metrics.path}\")\n   \
          \ print(f\"Training metrics: {metrics_dict}\")\n    # Add Prometheus metrics\
          \ server (after model evaluation)\n    prom_port = int(os.getenv(\"PROMETHEUS_PORT\"\
          , 8000))\n    start_http_server(prom_port)\n\n    # Create Prometheus gauges\n\
          \    train_r2_gauge = Gauge('train_r2_score', 'Training R\xB2 score', ['model_type'])\n\
          \    test_r2_gauge = Gauge('test_r2_score', 'Test R\xB2 score', ['model_type'])\n\
          \    training_time_gauge = Gauge('training_time_seconds', 'Training duration')\n\
          \n    # Set metric values\n    train_r2_gauge.labels(model_type=model_type).set(train_score)\n\
          \    test_r2_gauge.labels(model_type=model_type).set(test_score)\n    training_time_gauge.set(time.time()\
          \ - start_time)\n\n"
        env:
        - name: PROMETHEUS_PORT
          value: '8000'
        image: pes1ug19cs601/wine-quality-mlops:latest
    exec-train-3:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'xgboost' 'lightgbm'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train(\n    features: Input[Dataset],\n    labels: Input[Dataset],\n\
          \    hyperparameters: dict,\n    model: Output[Model],\n    metrics: Output[Metrics],\n\
          \    scaler: Input[Model],\n    model_type: str = \"RandomForest\"  # Default\
          \ argument moved to the end\n):\n    \"\"\"Train a model with support for\
          \ multiple algorithms.\"\"\"\n    import os\n    import joblib\n    import\
          \ json\n    import numpy as np\n    from sklearn.model_selection import\
          \ train_test_split\n    from sklearn.ensemble import RandomForestRegressor\n\
          \    import xgboost as xgb\n    import lightgbm as lgb\n    import mlflow\n\
          \    import time\n    from prometheus_client import start_http_server, Gauge\n\
          \n    # Load data\n    X = np.load(features.path)\n    y = np.load(labels.path)\n\
          \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(X,\
          \ y, test_size=0.2, random_state=42)\n\n    # Initialize model based on\
          \ model_type\n    print(f\"Training {model_type} model with hyperparameters:\
          \ {hyperparameters}\")\n    if model_type == \"RandomForest\":\n       \
          \ model_obj = RandomForestRegressor(**hyperparameters)\n    elif model_type\
          \ == \"XGBoost\":\n        model_obj = xgb.XGBRegressor(**hyperparameters)\n\
          \    elif model_type == \"LightGBM\":\n        model_obj = lgb.LGBMRegressor(**hyperparameters)\n\
          \    else:\n        raise ValueError(f\"Unsupported model type: {model_type}\"\
          )\n\n    start_time = time.time()\n    # Train model\n    model_obj.fit(X_train,\
          \ y_train)\n\n    # Evaluate model\n    train_score = model_obj.score(X_train,\
          \ y_train)\n    test_score = model_obj.score(X_test, y_test)\n\n    # Save\
          \ metrics\n    metrics_dict = {\n        'model_type': model_type,\n   \
          \     'train_r2': float(train_score),\n        'test_r2': float(test_score)\n\
          \    }\n\n    with open(metrics.path, 'w') as f:\n        json.dump(metrics_dict,\
          \ f, indent=2)\n\n    # Save model\n    joblib.dump(model_obj, model.path)\n\
          \n    # Only log to MLflow if not in testing mode and if MLflow server is\
          \ available\n    if not os.environ.get(\"TESTING\", \"False\").lower() ==\
          \ \"true\":\n        try:\n            # Set a timeout for MLflow connection\
          \ attempts\n            import socket\n            socket.setdefaulttimeout(5)\
          \  # 5 second timeout\n\n            # Try to connect to MLflow server\n\
          \            mlflow.set_tracking_uri('http://mlflow-service.mlops.svc.cluster.local:5000')\n\
          \            with mlflow.start_run():\n                mlflow.log_param(\"\
          model_type\", model_type)\n                mlflow.log_params(hyperparameters)\n\
          \                mlflow.log_metric(\"train_r2\", train_score)\n        \
          \        mlflow.log_metric(\"test_r2\", test_score)\n                mlflow.sklearn.log_model(model_obj,\
          \ \"model\")\n                mlflow.log_artifact(scaler.path, \"preprocessor\"\
          )\n                print(\"Successfully logged metrics to MLflow\")\n  \
          \      except Exception as e:\n            print(f\"MLflow logging failed\
          \ (this is expected in local environments): {e}\")\n            print(\"\
          Continuing without MLflow logging\")\n    else:\n        print(\"Testing\
          \ mode active, skipping MLflow logging\")\n\n    print(f\"Model saved to\
          \ {model.path}\")\n    print(f\"Metrics saved to {metrics.path}\")\n   \
          \ print(f\"Training metrics: {metrics_dict}\")\n    # Add Prometheus metrics\
          \ server (after model evaluation)\n    prom_port = int(os.getenv(\"PROMETHEUS_PORT\"\
          , 8000))\n    start_http_server(prom_port)\n\n    # Create Prometheus gauges\n\
          \    train_r2_gauge = Gauge('train_r2_score', 'Training R\xB2 score', ['model_type'])\n\
          \    test_r2_gauge = Gauge('test_r2_score', 'Test R\xB2 score', ['model_type'])\n\
          \    training_time_gauge = Gauge('training_time_seconds', 'Training duration')\n\
          \n    # Set metric values\n    train_r2_gauge.labels(model_type=model_type).set(train_score)\n\
          \    test_r2_gauge.labels(model_type=model_type).set(test_score)\n    training_time_gauge.set(time.time()\
          \ - start_time)\n\n"
        env:
        - name: PROMETHEUS_PORT
          value: '8000'
        image: pes1ug19cs601/wine-quality-mlops:latest
    exec-validate-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - validate_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\nfrom builtins import bool\n\ndef validate_data(\n    data_path: str,\n\
          \    metrics: Output[Metrics],\n    validation_success: Output[bool]\n):\n\
          \    \"\"\"Validate wine data for drift using Great Expectations with Git\
          \ functionality disabled.\"\"\"\n    import os\n    import sys\n    import\
          \ json\n    import pandas as pd\n\n    import logging\n    # Configure root\
          \ logger and specific loggers to suppress debug messages\n    logging.getLogger().setLevel(logging.ERROR)\
          \  # Set root logger to ERROR\n    logging.getLogger('great_expectations').setLevel(logging.ERROR)\n\
          \    logging.getLogger('great_expectations.expectations.registry').setLevel(logging.CRITICAL)\
          \  # Even stricter for registry\n\n\n    # Set environment variables\n \
          \   os.environ[\"GE_USAGE_STATISTICS_ENABLED\"] = \"False\"\n    os.environ[\"\
          GE_UNCOMMITTED_DIRECTORIES\"] = \"True\"\n    os.environ[\"GX_ASSUME_MISSING_LIBRARIES\"\
          ] = \"git\"\n\n\n    # Import Great Expectations components\n    from great_expectations.data_context.types.base\
          \ import DataContextConfig\n    from great_expectations.data_context import\
          \ BaseDataContext\n    from great_expectations.core.batch import RuntimeBatchRequest\n\
          \n    try:\n        # Load data\n        print(\"Loading data...\")\n  \
          \      df = pd.read_csv(data_path)\n\n        # Create context configuration\n\
          \        print(\"Creating context configuration...\")\n        context_config\
          \ = DataContextConfig(\n            store_backend_defaults=None,\n     \
          \       checkpoint_store_name=None,\n            datasources={\n       \
          \         \"pandas_datasource\": {\n                    \"class_name\":\
          \ \"Datasource\",\n                    \"module_name\": \"great_expectations.datasource\"\
          ,\n                    \"execution_engine\": {\n                       \
          \ \"class_name\": \"PandasExecutionEngine\",\n                        \"\
          module_name\": \"great_expectations.execution_engine\"\n               \
          \     },\n                    \"data_connectors\": {\n                 \
          \       \"runtime_connector\": {\n                            \"class_name\"\
          : \"RuntimeDataConnector\",\n                            \"module_name\"\
          : \"great_expectations.datasource.data_connector\",\n                  \
          \          \"batch_identifiers\": [\"batch_id\"]\n                     \
          \   }\n                    }\n                }\n            }\n       \
          \ )\n\n        # Create context\n        print(\"Creating context...\")\n\
          \        context = BaseDataContext(project_config=context_config)\n\n  \
          \      # Create expectation suite\n        print(\"Creating expectation\
          \ suite...\")\n        suite_name = \"wine_quality_suite\"\n        context.create_expectation_suite(suite_name,\
          \ overwrite_existing=True)\n\n        # Create batch request\n        print(\"\
          Creating batch request...\")\n        batch_request = RuntimeBatchRequest(\n\
          \            datasource_name=\"pandas_datasource\",\n            data_connector_name=\"\
          runtime_connector\",\n            data_asset_name=\"wine_data\",\n     \
          \       runtime_parameters={\"batch_data\": df},\n            batch_identifiers={\"\
          batch_id\": \"default_identifier\"},\n        )\n\n        # Get validator\n\
          \        print(\"Getting validator...\")\n        validator = context.get_validator(\n\
          \            batch_request=batch_request,\n            expectation_suite_name=suite_name\n\
          \        )\n\n        # Add expectations\n        print(\"Adding expectations...\"\
          )\n        expectations = []\n\n        # Check that columns match expected\
          \ list\n        expectations.append(validator.expect_table_columns_to_match_ordered_list(list(df.columns)))\n\
          \n        # Check data types\n        expectations.append(validator.expect_column_values_to_be_of_type(\"\
          fixed acidity\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"\
          volatile acidity\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"\
          citric acid\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"\
          residual sugar\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"\
          chlorides\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"\
          free sulfur dioxide\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"\
          total sulfur dioxide\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"\
          density\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"\
          pH\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"\
          sulphates\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"\
          alcohol\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"\
          quality\", \"int64\"))\n\n        # Check value ranges\n        expectations.append(validator.expect_column_values_to_be_between(\"\
          fixed acidity\", min_value=3.8, max_value=15.9))\n        expectations.append(validator.expect_column_values_to_be_between(\"\
          volatile acidity\", min_value=0.08, max_value=1.58))\n        expectations.append(validator.expect_column_values_to_be_between(\"\
          citric acid\", min_value=0, max_value=1.66))\n        expectations.append(validator.expect_column_values_to_be_between(\"\
          residual sugar\", min_value=0.6, max_value=65.8))\n        expectations.append(validator.expect_column_values_to_be_between(\"\
          chlorides\", min_value=0.009, max_value=0.611))\n        expectations.append(validator.expect_column_values_to_be_between(\"\
          free sulfur dioxide\", min_value=1, max_value=289))\n        expectations.append(validator.expect_column_values_to_be_between(\"\
          total sulfur dioxide\", min_value=6, max_value=440))\n        expectations.append(validator.expect_column_values_to_be_between(\"\
          density\", min_value=0.98711, max_value=1.03898))\n        expectations.append(validator.expect_column_values_to_be_between(\"\
          pH\", min_value=2.72, max_value=4.01))\n        expectations.append(validator.expect_column_values_to_be_between(\"\
          sulphates\", min_value=0.22, max_value=2))\n        expectations.append(validator.expect_column_values_to_be_between(\"\
          alcohol\", min_value=8, max_value=14.9))\n        expectations.append(validator.expect_column_values_to_be_between(\"\
          quality\", min_value=3, max_value=9))\n\n        # Check for missing values\n\
          \        for column in df.columns:\n            expectations.append(validator.expect_column_values_to_not_be_null(column))\n\
          \n        # Run validation\n        print(\"Running validation...\")\n \
          \       validation_results = validator.validate()\n\n        # Process results\n\
          \        print(\"Processing results...\")\n        validation_passed = validation_results.success\n\
          \n        # Prepare metrics\n        validation_metrics = {\n          \
          \  \"validation_success\": validation_passed,\n            \"evaluated_expectations\"\
          : validation_results.statistics[\"evaluated_expectations\"],\n         \
          \   \"successful_expectations\": validation_results.statistics[\"successful_expectations\"\
          ],\n            \"unsuccessful_expectations\": validation_results.statistics[\"\
          unsuccessful_expectations\"],\n        }\n\n        # Log metrics\n    \
          \    print(\"Logging metrics...\")\n        metrics.log_metric(\"validation_success\"\
          , float(validation_passed))\n        metrics.log_metric(\"evaluated_expectations\"\
          , float(validation_results.statistics[\"evaluated_expectations\"]))\n  \
          \      metrics.log_metric(\"successful_expectations\", float(validation_results.statistics[\"\
          successful_expectations\"]))\n        metrics.log_metric(\"unsuccessful_expectations\"\
          , float(validation_results.statistics[\"unsuccessful_expectations\"]))\n\
          \n        print(f\"Validation {'passed' if validation_passed else 'failed'}\"\
          )\n        print(f\"Metrics: {validation_metrics}\")\n\n        validation_success\
          \ = validation_passed\n        with open(validation_success.path, 'w') as\
          \ f:\n            f.write(str(validation_passed).lower())\n        return\
          \ validation_metrics\n\n    except Exception as e:\n        print(f\"Error\
          \ in validate_data: {type(e).__name__}: {str(e)}\")\n        import traceback\n\
          \        print(traceback.format_exc())\n        # Log failure in metrics\n\
          \        metrics.log_metric(\"validation_success\", 0.0)\n        metrics.log_metric(\"\
          error\", 1.0)\n        with open(validation_success.path, 'w') as f:\n \
          \           f.write(\"False\")\n        raise\n\n"
        image: pes1ug19cs601/wine-quality-mlops:latest
pipelineInfo:
  description: End-to-end ML pipeline for wine quality prediction with model selection
  name: wine-quality-pipeline
root:
  dag:
    tasks:
      condition-1:
        componentRef:
          name: comp-condition-1
        dependentTasks:
        - validate-data
        inputs:
          parameters:
            pipelinechannel--data_path:
              componentInputParameter: data_path
            pipelinechannel--lgbm_learning_rate:
              componentInputParameter: lgbm_learning_rate
            pipelinechannel--lgbm_max_depth:
              componentInputParameter: lgbm_max_depth
            pipelinechannel--lgbm_n_estimators:
              componentInputParameter: lgbm_n_estimators
            pipelinechannel--rf_max_depth:
              componentInputParameter: rf_max_depth
            pipelinechannel--rf_min_samples_split:
              componentInputParameter: rf_min_samples_split
            pipelinechannel--rf_n_estimators:
              componentInputParameter: rf_n_estimators
            pipelinechannel--service_name:
              componentInputParameter: service_name
            pipelinechannel--validate-data-validation_success:
              taskOutputParameter:
                outputParameterKey: validation_success
                producerTask: validate-data
            pipelinechannel--xgb_learning_rate:
              componentInputParameter: xgb_learning_rate
            pipelinechannel--xgb_max_depth:
              componentInputParameter: xgb_max_depth
            pipelinechannel--xgb_n_estimators:
              componentInputParameter: xgb_n_estimators
        taskInfo:
          name: condition-1
        triggerPolicy:
          condition: inputs.parameter_values['pipelinechannel--validate-data-validation_success']
            == true
      validate-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-validate-data
        inputs:
          parameters:
            data_path:
              componentInputParameter: data_path
        taskInfo:
          name: validate-data
  inputDefinitions:
    parameters:
      data_path:
        defaultValue: https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv
        isOptional: true
        parameterType: STRING
      lgbm_learning_rate:
        defaultValue: 0.1
        isOptional: true
        parameterType: NUMBER_DOUBLE
      lgbm_max_depth:
        defaultValue: -1.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      lgbm_n_estimators:
        defaultValue: 100.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      rf_max_depth:
        isOptional: true
        parameterType: NUMBER_INTEGER
      rf_min_samples_split:
        defaultValue: 2.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      rf_n_estimators:
        defaultValue: 100.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      service_name:
        defaultValue: wine-quality-predictor
        isOptional: true
        parameterType: STRING
      use_lightgbm:
        defaultValue: true
        isOptional: true
        parameterType: BOOLEAN
      use_random_forest:
        defaultValue: true
        isOptional: true
        parameterType: BOOLEAN
      use_xgboost:
        defaultValue: true
        isOptional: true
        parameterType: BOOLEAN
      xgb_learning_rate:
        defaultValue: 0.1
        isOptional: true
        parameterType: NUMBER_DOUBLE
      xgb_max_depth:
        defaultValue: 6.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      xgb_n_estimators:
        defaultValue: 100.0
        isOptional: true
        parameterType: NUMBER_INTEGER
schemaVersion: 2.1.0
sdkVersion: kfp-2.12.1
