# PIPELINE DEFINITION
# Name: wine-quality-pipeline
# Description: End-to-end ML pipeline for wine quality prediction
# Inputs:
#    data_path: str [Default: 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv']
#    max_depth: int
#    min_samples_leaf: int [Default: 1.0]
#    min_samples_split: int [Default: 2.0]
#    n_estimators: int [Default: 100.0]
#    random_state: int [Default: 42.0]
#    service_name: str [Default: 'wine-quality-predictor']
components:
  comp-condition-1:
    dag:
      tasks:
        deploy-model:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-deploy-model
          dependentTasks:
          - preprocess
          - train
          inputs:
            artifacts:
              metrics:
                taskOutputArtifact:
                  outputArtifactKey: metrics
                  producerTask: train
              model:
                taskOutputArtifact:
                  outputArtifactKey: model
                  producerTask: train
              scaler:
                taskOutputArtifact:
                  outputArtifactKey: scaler
                  producerTask: preprocess
            parameters:
              service_name:
                componentInputParameter: pipelinechannel--service_name
          taskInfo:
            name: deploy-model
        preprocess:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-preprocess
          inputs:
            parameters:
              data_path:
                componentInputParameter: pipelinechannel--data_path
          taskInfo:
            name: preprocess
        train:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-train
          dependentTasks:
          - preprocess
          inputs:
            artifacts:
              features:
                taskOutputArtifact:
                  outputArtifactKey: features
                  producerTask: preprocess
              labels:
                taskOutputArtifact:
                  outputArtifactKey: labels
                  producerTask: preprocess
              scaler:
                taskOutputArtifact:
                  outputArtifactKey: scaler
                  producerTask: preprocess
            parameters:
              hyperparameters:
                runtimeValue:
                  constant:
                    max_depth: '{{$.inputs.parameters[''pipelinechannel--max_depth'']}}'
                    min_samples_leaf: '{{$.inputs.parameters[''pipelinechannel--min_samples_leaf'']}}'
                    min_samples_split: '{{$.inputs.parameters[''pipelinechannel--min_samples_split'']}}'
                    n_estimators: '{{$.inputs.parameters[''pipelinechannel--n_estimators'']}}'
                    random_state: '{{$.inputs.parameters[''pipelinechannel--random_state'']}}'
              pipelinechannel--max_depth:
                componentInputParameter: pipelinechannel--max_depth
              pipelinechannel--min_samples_leaf:
                componentInputParameter: pipelinechannel--min_samples_leaf
              pipelinechannel--min_samples_split:
                componentInputParameter: pipelinechannel--min_samples_split
              pipelinechannel--n_estimators:
                componentInputParameter: pipelinechannel--n_estimators
              pipelinechannel--random_state:
                componentInputParameter: pipelinechannel--random_state
          taskInfo:
            name: train
    inputDefinitions:
      parameters:
        pipelinechannel--data_path:
          parameterType: STRING
        pipelinechannel--max_depth:
          parameterType: NUMBER_INTEGER
        pipelinechannel--min_samples_leaf:
          parameterType: NUMBER_INTEGER
        pipelinechannel--min_samples_split:
          parameterType: NUMBER_INTEGER
        pipelinechannel--n_estimators:
          parameterType: NUMBER_INTEGER
        pipelinechannel--random_state:
          parameterType: NUMBER_INTEGER
        pipelinechannel--service_name:
          parameterType: STRING
        pipelinechannel--validate-data-validation_success:
          parameterType: BOOLEAN
  comp-deploy-model:
    executorLabel: exec-deploy-model
    inputDefinitions:
      artifacts:
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        scaler:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        namespace:
          defaultValue: kubeflow
          isOptional: true
          parameterType: STRING
        service_name:
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-preprocess:
    executorLabel: exec-preprocess
    inputDefinitions:
      parameters:
        data_path:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        features:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        labels:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        scaler:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-train:
    executorLabel: exec-train
    inputDefinitions:
      artifacts:
        features:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        labels:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        scaler:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        hyperparameters:
          parameterType: STRUCT
    outputDefinitions:
      artifacts:
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-validate-data:
    executorLabel: exec-validate-data
    inputDefinitions:
      parameters:
        data_path:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
      parameters:
        validation_success:
          parameterType: BOOLEAN
deploymentSpec:
  executors:
    exec-deploy-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - deploy_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'kserve==0.10.1'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef deploy_model(\n    model: Input[Model],\n    metrics: Input[Metrics],\n\
          \    scaler: Input[Model],\n    service_name: str,\n    namespace: str =\
          \ \"kubeflow\"\n) -> str:\n    import os\n    import json\n    import pickle\n\
          \    import tempfile\n    from kubernetes import client\n    from kubernetes\
          \ import config\n    from kserve import KServeClient\n    from kserve import\
          \ constants\n    from kserve import V1beta1InferenceService\n    from kserve\
          \ import V1beta1InferenceServiceSpec\n    from kserve import V1beta1PredictorSpec\n\
          \    from kserve import V1beta1SKLearnSpec\n\n    # Create a temporary directory\
          \ to prepare the model\n    model_dir = tempfile.mkdtemp()\n    os.makedirs(os.path.join(model_dir,\
          \ \"model\"), exist_ok=True)\n\n    # Load and save the model and scaler\n\
          \    with open(model.path, 'rb') as f:\n        model_obj = pickle.load(f)\n\
          \n    with open(scaler.path, 'rb') as f:\n        scaler_obj = pickle.load(f)\n\
          \n    # Save model and scaler to the temporary directory\n    with open(os.path.join(model_dir,\
          \ \"model\", \"model.pkl\"), 'wb') as f:\n        pickle.dump(model_obj,\
          \ f)\n\n    with open(os.path.join(model_dir, \"model\", \"scaler.pkl\"\
          ), 'wb') as f:\n        pickle.dump(scaler_obj, f)\n\n    # Load metrics\
          \ to include in model metadata\n    with open(metrics.path, 'r') as f:\n\
          \        metrics_data = json.load(f)\n\n    # Create a metadata file\n \
          \   metadata = {\n        \"name\": service_name,\n        \"version\":\
          \ \"v1\",\n        \"metrics\": metrics_data\n    }\n\n    with open(os.path.join(model_dir,\
          \ \"model\", \"metadata.json\"), 'w') as f:\n        json.dump(metadata,\
          \ f)\n\n    # Create a simple inference script\n    inference_script = \"\
          \"\"\nimport os\nimport pickle\nimport json\nimport numpy as np\n\nclass\
          \ WineQualityModel(object):\n    def __init__(self):\n        self.model\
          \ = None\n        self.scaler = None\n        self.ready = False\n\n   \
          \ def load(self):\n        model_dir = os.path.join(os.getcwd(), \"model\"\
          )\n        with open(os.path.join(model_dir, \"model.pkl\"), \"rb\") as\
          \ f:\n            self.model = pickle.load(f)\n        with open(os.path.join(model_dir,\
          \ \"scaler.pkl\"), \"rb\") as f:\n            self.scaler = pickle.load(f)\n\
          \        self.ready = True\n\n    def predict(self, X, feature_names=None):\n\
          \        if not self.ready:\n            self.load()\n        X_scaled =\
          \ self.scaler.transform(X)\n        predictions = self.model.predict(X_scaled)\n\
          \        return predictions.tolist()\n\"\"\"\n\n    with open(os.path.join(model_dir,\
          \ \"model\", \"WineQualityModel.py\"), 'w') as f:\n        f.write(inference_script)\n\
          \n    # Create a simple requirements.txt\n    with open(os.path.join(model_dir,\
          \ \"model\", \"requirements.txt\"), 'w') as f:\n        f.write(\"scikit-learn==1.0.2\\\
          nnumpy==1.22.3\\n\")\n\n    # Upload the model to a storage location (MinIO,\
          \ S3, etc.)\n    # For this example, we'll assume you have a PVC for model\
          \ storage\n    model_uri = f\"pvc://{service_name}-models\"\n\n    # In\
          \ a real implementation, you would upload the model to your storage\n  \
          \  # For now, we'll just print the path and assume it's accessible\n   \
          \ print(f\"Model prepared at: {model_dir}\")\n    print(f\"Model would be\
          \ deployed from: {model_uri}\")\n\n    try:\n        # Initialize KServe\
          \ client\n        config.load_incluster_config()\n        kserve_client\
          \ = KServeClient()\n\n        # Define the inference service\n        isvc\
          \ = V1beta1InferenceService(\n            api_version=constants.KSERVE_V1BETA1,\n\
          \            kind=constants.KSERVE_KIND,\n            metadata=client.V1ObjectMeta(\n\
          \                name=service_name,\n                namespace=namespace,\n\
          \                annotations={\"sidecar.istio.io/inject\": \"false\"}\n\
          \            ),\n            spec=V1beta1InferenceServiceSpec(\n       \
          \         predictor=V1beta1PredictorSpec(\n                    sklearn=V1beta1SKLearnSpec(\n\
          \                        storage_uri=model_uri,\n                      \
          \  resources=client.V1ResourceRequirements(\n                          \
          \  requests={\"cpu\": \"100m\", \"memory\": \"1Gi\"},\n                \
          \            limits={\"cpu\": \"1\", \"memory\": \"2Gi\"}\n            \
          \            )\n                    )\n                )\n            )\n\
          \        )\n\n        # Create the inference service\n        kserve_client.create(isvc)\n\
          \        print(f\"Inference service '{service_name}' created in namespace\
          \ '{namespace}'\")\n\n        # Get the URL of the deployed service\n  \
          \      service_url = f\"http://{service_name}.{namespace}.svc.cluster.local/v1/models/{service_name}:predict\"\
          \n        return service_url\n\n    except Exception as e:\n        print(f\"\
          Error deploying model: {str(e)}\")\n        return f\"Deployment failed:\
          \ {str(e)}\"\n\n"
        image: pes1ug19cs601/wine-quality-mlops:latest
    exec-preprocess:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - preprocess
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef preprocess(\n    data_path: str,\n    features: Output[Dataset],\n\
          \    labels: Output[Dataset],\n    scaler: Output[Model]\n):\n    \"\"\"\
          Preprocess the wine quality data.\"\"\"\n    # Load data\n    df = pd.read_csv(data_path,\
          \ sep=\";\")\n\n    # Split features and labels\n    X = df.drop('quality',\
          \ axis=1)\n    y = df['quality']\n\n    # Scale features\n    scaler_obj\
          \ = StandardScaler()\n    X_scaled = scaler_obj.fit_transform(X)\n\n   \
          \ # Save processed data\n    np.save(features.path, X_scaled)\n    np.save(labels.path,\
          \ y.values)\n    joblib.dump(scaler_obj, scaler.path)\n\n    print(f\"Preprocessed\
          \ features saved to {features.path}\")\n    print(f\"Preprocessed labels\
          \ saved to {labels.path}\")\n    print(f\"Scaler saved to {scaler.path}\"\
          )\n\n"
        image: pes1ug19cs601/wine-quality-mlops:latest
    exec-train:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train(\n    features: Input[Dataset],\n    labels: Input[Dataset],\n\
          \    hyperparameters: dict,\n    model: Output[Model],\n    metrics: Output[Metrics],\n\
          \    scaler: Input[Model]\n):\n    \"\"\"Train a RandomForestRegressor model.\"\
          \"\"\n    import os\n    import joblib\n    import json\n    import numpy\
          \ as np\n    from sklearn.model_selection import train_test_split\n    from\
          \ sklearn.ensemble import RandomForestRegressor\n    import mlflow\n\n \
          \   # Load data\n    X = np.load(features.path)\n    y = np.load(labels.path)\n\
          \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(X,\
          \ y, test_size=0.2, random_state=42)\n\n    # Train model\n    model_obj\
          \ = RandomForestRegressor(**hyperparameters)\n    model_obj.fit(X_train,\
          \ y_train)\n\n    # Evaluate model\n    train_score = model_obj.score(X_train,\
          \ y_train)\n    test_score = model_obj.score(X_test, y_test)\n\n    # Save\
          \ metrics\n    metrics_dict = {\n        'train_r2': float(train_score),\n\
          \        'test_r2': float(test_score)\n    }\n    with open(metrics.path,\
          \ 'w') as f:\n        json.dump(metrics_dict, f, indent=2)\n\n    # Save\
          \ model\n    joblib.dump(model_obj, model.path)\n\n    # Only log to MLflow\
          \ if not in testing mode and if MLflow server is available\n    if not os.environ.get(\"\
          TESTING\", \"False\").lower() == \"true\":\n        try:\n            #\
          \ Set a timeout for MLflow connection attempts\n            import socket\n\
          \            socket.setdefaulttimeout(5)  # 5 second timeout\n\n       \
          \     # Try to connect to MLflow server\n            mlflow.set_tracking_uri('http://mlflow-service.mlops.svc.cluster.local:5000')\n\
          \n            with mlflow.start_run():\n                mlflow.log_params(hyperparameters)\n\
          \                mlflow.log_metric(\"train_r2\", train_score)\n        \
          \        mlflow.log_metric(\"test_r2\", test_score)\n                mlflow.sklearn.log_model(model_obj,\
          \ \"model\")\n                mlflow.log_artifact(scaler.path, \"preprocessor\"\
          )\n\n            print(\"Successfully logged metrics to MLflow\")\n    \
          \    except Exception as e:\n            print(f\"MLflow logging failed\
          \ (this is expected in local environments): {e}\")\n            print(\"\
          Continuing without MLflow logging\")\n    else:\n        print(\"Testing\
          \ mode active, skipping MLflow logging\")\n\n    print(f\"Model saved to\
          \ {model.path}\")\n    print(f\"Metrics saved to {metrics.path}\")\n   \
          \ print(f\"Training metrics: {metrics_dict}\")\n\n"
        image: pes1ug19cs601/wine-quality-mlops:latest
    exec-validate-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - validate_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\nfrom builtins import bool\n\ndef validate_data(\n    data_path: str,\n\
          \    metrics: Output[Metrics],\n    validation_success: Output[bool]\n):\n\
          \    \"\"\"Validate wine data for drift using Great Expectations with Git\
          \ functionality disabled.\"\"\"\n    import os\n    import sys\n    import\
          \ json\n    import pandas as pd\n\n    import logging\n    # Configure root\
          \ logger and specific loggers to suppress debug messages\n    logging.getLogger().setLevel(logging.ERROR)\
          \  # Set root logger to ERROR\n    logging.getLogger('great_expectations').setLevel(logging.ERROR)\n\
          \    logging.getLogger('great_expectations.expectations.registry').setLevel(logging.CRITICAL)\
          \  # Even stricter for registry\n\n\n    # Set environment variables\n \
          \   os.environ[\"GE_USAGE_STATISTICS_ENABLED\"] = \"False\"\n    os.environ[\"\
          GE_UNCOMMITTED_DIRECTORIES\"] = \"True\"\n    os.environ[\"GX_ASSUME_MISSING_LIBRARIES\"\
          ] = \"git\"\n\n\n    # Import Great Expectations components\n    from great_expectations.data_context.types.base\
          \ import DataContextConfig\n    from great_expectations.data_context import\
          \ BaseDataContext\n    from great_expectations.core.batch import RuntimeBatchRequest\n\
          \n    try:\n        # Load data\n        print(\"Loading data...\")\n  \
          \      df = pd.read_csv(data_path)\n\n        # Create context configuration\n\
          \        print(\"Creating context configuration...\")\n        context_config\
          \ = DataContextConfig(\n            store_backend_defaults=None,\n     \
          \       checkpoint_store_name=None,\n            datasources={\n       \
          \         \"pandas_datasource\": {\n                    \"class_name\":\
          \ \"Datasource\",\n                    \"module_name\": \"great_expectations.datasource\"\
          ,\n                    \"execution_engine\": {\n                       \
          \ \"class_name\": \"PandasExecutionEngine\",\n                        \"\
          module_name\": \"great_expectations.execution_engine\"\n               \
          \     },\n                    \"data_connectors\": {\n                 \
          \       \"runtime_connector\": {\n                            \"class_name\"\
          : \"RuntimeDataConnector\",\n                            \"module_name\"\
          : \"great_expectations.datasource.data_connector\",\n                  \
          \          \"batch_identifiers\": [\"batch_id\"]\n                     \
          \   }\n                    }\n                }\n            }\n       \
          \ )\n\n        # Create context\n        print(\"Creating context...\")\n\
          \        context = BaseDataContext(project_config=context_config)\n\n  \
          \      # Create expectation suite\n        print(\"Creating expectation\
          \ suite...\")\n        suite_name = \"wine_quality_suite\"\n        context.create_expectation_suite(suite_name,\
          \ overwrite_existing=True)\n\n        # Create batch request\n        print(\"\
          Creating batch request...\")\n        batch_request = RuntimeBatchRequest(\n\
          \            datasource_name=\"pandas_datasource\",\n            data_connector_name=\"\
          runtime_connector\",\n            data_asset_name=\"wine_data\",\n     \
          \       runtime_parameters={\"batch_data\": df},\n            batch_identifiers={\"\
          batch_id\": \"default_identifier\"},\n        )\n\n        # Get validator\n\
          \        print(\"Getting validator...\")\n        validator = context.get_validator(\n\
          \            batch_request=batch_request,\n            expectation_suite_name=suite_name\n\
          \        )\n\n        # Add expectations\n        print(\"Adding expectations...\"\
          )\n        expectations = []\n\n        # Check that columns match expected\
          \ list\n        expectations.append(validator.expect_table_columns_to_match_ordered_list(list(df.columns)))\n\
          \n        # Check data types\n        expectations.append(validator.expect_column_values_to_be_of_type(\"\
          fixed acidity\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"\
          volatile acidity\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"\
          citric acid\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"\
          residual sugar\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"\
          chlorides\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"\
          free sulfur dioxide\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"\
          total sulfur dioxide\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"\
          density\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"\
          pH\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"\
          sulphates\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"\
          alcohol\", \"float64\"))\n        expectations.append(validator.expect_column_values_to_be_of_type(\"\
          quality\", \"int64\"))\n\n        # Check value ranges\n        expectations.append(validator.expect_column_values_to_be_between(\"\
          fixed acidity\", min_value=3.8, max_value=15.9))\n        expectations.append(validator.expect_column_values_to_be_between(\"\
          volatile acidity\", min_value=0.08, max_value=1.58))\n        expectations.append(validator.expect_column_values_to_be_between(\"\
          citric acid\", min_value=0, max_value=1.66))\n        expectations.append(validator.expect_column_values_to_be_between(\"\
          residual sugar\", min_value=0.6, max_value=65.8))\n        expectations.append(validator.expect_column_values_to_be_between(\"\
          chlorides\", min_value=0.009, max_value=0.611))\n        expectations.append(validator.expect_column_values_to_be_between(\"\
          free sulfur dioxide\", min_value=1, max_value=289))\n        expectations.append(validator.expect_column_values_to_be_between(\"\
          total sulfur dioxide\", min_value=6, max_value=440))\n        expectations.append(validator.expect_column_values_to_be_between(\"\
          density\", min_value=0.98711, max_value=1.03898))\n        expectations.append(validator.expect_column_values_to_be_between(\"\
          pH\", min_value=2.72, max_value=4.01))\n        expectations.append(validator.expect_column_values_to_be_between(\"\
          sulphates\", min_value=0.22, max_value=2))\n        expectations.append(validator.expect_column_values_to_be_between(\"\
          alcohol\", min_value=8, max_value=14.9))\n        expectations.append(validator.expect_column_values_to_be_between(\"\
          quality\", min_value=3, max_value=9))\n\n        # Check for missing values\n\
          \        for column in df.columns:\n            expectations.append(validator.expect_column_values_to_not_be_null(column))\n\
          \n        # Run validation\n        print(\"Running validation...\")\n \
          \       validation_results = validator.validate()\n\n        # Process results\n\
          \        print(\"Processing results...\")\n        validation_passed = validation_results.success\n\
          \n        # Prepare metrics\n        validation_metrics = {\n          \
          \  \"validation_success\": validation_passed,\n            \"evaluated_expectations\"\
          : validation_results.statistics[\"evaluated_expectations\"],\n         \
          \   \"successful_expectations\": validation_results.statistics[\"successful_expectations\"\
          ],\n            \"unsuccessful_expectations\": validation_results.statistics[\"\
          unsuccessful_expectations\"],\n        }\n\n        # Log metrics\n    \
          \    print(\"Logging metrics...\")\n        metrics.log_metric(\"validation_success\"\
          , float(validation_passed))\n        metrics.log_metric(\"evaluated_expectations\"\
          , float(validation_results.statistics[\"evaluated_expectations\"]))\n  \
          \      metrics.log_metric(\"successful_expectations\", float(validation_results.statistics[\"\
          successful_expectations\"]))\n        metrics.log_metric(\"unsuccessful_expectations\"\
          , float(validation_results.statistics[\"unsuccessful_expectations\"]))\n\
          \n        print(f\"Validation {'passed' if validation_passed else 'failed'}\"\
          )\n        print(f\"Metrics: {validation_metrics}\")\n\n        validation_success\
          \ = validation_passed\n        with open(validation_success.path, 'w') as\
          \ f:\n            f.write(str(validation_passed).lower())\n        return\
          \ validation_metrics\n\n    except Exception as e:\n        print(f\"Error\
          \ in validate_data: {type(e).__name__}: {str(e)}\")\n        import traceback\n\
          \        print(traceback.format_exc())\n        # Log failure in metrics\n\
          \        metrics.log_metric(\"validation_success\", 0.0)\n        metrics.log_metric(\"\
          error\", 1.0)\n        with open(validation_success.path, 'w') as f:\n \
          \           f.write(\"False\")\n        raise\n\n"
        image: pes1ug19cs601/wine-quality-mlops:latest
pipelineInfo:
  description: End-to-end ML pipeline for wine quality prediction
  name: wine-quality-pipeline
root:
  dag:
    tasks:
      condition-1:
        componentRef:
          name: comp-condition-1
        dependentTasks:
        - validate-data
        inputs:
          parameters:
            pipelinechannel--data_path:
              componentInputParameter: data_path
            pipelinechannel--max_depth:
              componentInputParameter: max_depth
            pipelinechannel--min_samples_leaf:
              componentInputParameter: min_samples_leaf
            pipelinechannel--min_samples_split:
              componentInputParameter: min_samples_split
            pipelinechannel--n_estimators:
              componentInputParameter: n_estimators
            pipelinechannel--random_state:
              componentInputParameter: random_state
            pipelinechannel--service_name:
              componentInputParameter: service_name
            pipelinechannel--validate-data-validation_success:
              taskOutputParameter:
                outputParameterKey: validation_success
                producerTask: validate-data
        taskInfo:
          name: condition-1
        triggerPolicy:
          condition: inputs.parameter_values['pipelinechannel--validate-data-validation_success']
            == true
      validate-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-validate-data
        inputs:
          parameters:
            data_path:
              componentInputParameter: data_path
        taskInfo:
          name: validate-data
  inputDefinitions:
    parameters:
      data_path:
        defaultValue: https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv
        isOptional: true
        parameterType: STRING
      max_depth:
        isOptional: true
        parameterType: NUMBER_INTEGER
      min_samples_leaf:
        defaultValue: 1.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      min_samples_split:
        defaultValue: 2.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      n_estimators:
        defaultValue: 100.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      random_state:
        defaultValue: 42.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      service_name:
        defaultValue: wine-quality-predictor
        isOptional: true
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.12.1
